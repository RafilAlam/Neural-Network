# üß† Neural Network Layer Implementation  
üî® **Building Blocks for Large Prediction Models**  
> This is a simple **Layer** implementation that includes various activation functions (sigmoid, tanh, ReLU, leaky ReLU, softmax). It serves as the foundational layer for forward and backward propagation in a neural network.

## üõ†Ô∏è Features
- Implements commonly used activation functions:  
  - **Sigmoid**  
  - **Tanh**  
  - **ReLU**  
  - **Leaky ReLU**  
  - **Softmax**
- **Layer** class for forward propagation using specified activation function  
- Placeholder for backward propagation (gradient calculation)  

---

## üì¶ Tech Stack
**Language**: Python
- **NumPy**
